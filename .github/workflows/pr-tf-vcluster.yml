# GH Repo SECRETS Needed to be:

  # change region from "us-east-1" to the new one
  # change profile name from "bt-platform-test-eks-eph" to the new one
  # TF_REMOTE_REPO --> ONLY the name of remote-repo without .git (like test-tf), remote repo should be under same github account
  # AWS_ACCESS_KEY_ID_TEST1_V
  # AWS_SECRET_ACCESS_KEY_TEST1_V
  # ECR_IMAGE_REPO
  # EKS_TEST_CLUSTER_NAME
  # SSH_PRI --> SSH agent to access Remote Repo in same github account
  # BASTION_PRIVATE_KEY --> the pem content from bastion-host key pair
  # create (s3 & ecr)
  # INSTALL PSQL-CLient on Bastion-Host 

# VCluster_Prerequisties:
#  https://loft.sh/blog/aws-eks-multi-tenancy-with-vcluster/
#   eksctl utils associate-iam-oidc-provider --cluster $cluster_name --approve
#   eksctl create iamserviceaccount \
#       --name ebs-csi-controller-sa \
#       --namespace kube-system \
#       --cluster $cluster_name \
#       --role-name AmazonEKS_EBS_CSI_DriverRole \
#       --role-only \
#       --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
#       --approve
#   eksctl create addon --name aws-ebs-csi-driver --cluster $cluster_name --service-account-role-arn arn:aws:iam::<YOUR_ACCOUNT_NUMBER>:role/AmazonEKS_EBS_CSI_DriverRole --force

### NOTE:
  # Bastion Host --> SG. inbound & outbound rules ---> inbound open allowed from 0.0.0.0 otherwise use runs-on: self-hosted

  name: Provision Eph.Env

  on: #workflow_dispatch
    pull_request:
      types:
        - opened
  
  jobs:
    Provision-SQS-Lambda-VCluster:
      runs-on: ubuntu-latest #                runs-on: self-hosted
      outputs:
        VCLUSTER_ENDPOINT_URL: ${{ steps.connect-vcluster.outputs.VCLUSTER_ENDPOINT_URL }}

      steps:
        - name: Checkout Source Code Repository
          uses: actions/checkout@v3 #####v2
  
        - name: Set up SSH agent for Clone TF Remote Repo
          uses: webfactory/ssh-agent@v0.5.4
          with:
            ssh-private-key: ${{ secrets.SSH_PRI }}
  
        - name: Clone & Copy TF-Modules from Remote Repository #
          run: |
            TF_REMOTE_REPO="${{ secrets.TF_REMOTE_REPO }}"
            git clone --depth=1 git@github.com:${{ github.repository_owner }}/${TF_REMOTE_REPO}.git temp_repo
            mv temp_repo/modules . && rm -rf temp_repo
          
        - name: Configure AWS credentials
          uses: aws-actions/configure-aws-credentials@v1
          with:
            aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_TEST1_V }}
            aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_TEST1_V }}
            aws-region: us-east-1
            KUBECONFIG: ~/.kube/config
  
        - name: Login to Amazon ECR
          id: login-ecr
          uses: aws-actions/amazon-ecr-login@v1
 
        - name: Docker Build-Push
          run: |
            ecr_registry=${{ steps.login-ecr.outputs.registry }}
            ecr_repository=${{ secrets.ECR_IMAGE_REPO }}
            image_tag=eepr${{ github.event.number }}-${{ github.head_ref }}
            
            docker build -t $ecr_registry/$ecr_repository:$image_tag -f ./Dockerfile .
            docker tag $ecr_registry/$ecr_repository:$image_tag $ecr_registry/$ecr_repository:latest
            docker push $ecr_registry/$ecr_repository:$image_tag
  
        - name: Replace PR-No inside TFVars & S3-TF-StateFile
          run: |
            sed -i "s/GHPRNO/${{ github.event.number }}/g" tf-config/main.tf         #include GH-PR-No in S3 TFState Name 
            sed -i "s/GHPRNO/${{ github.event.number }}/g" tf-config/stg.tfvars      #include GH-PR-No in components Name
            
        - name: Terraform Init to Apply Resources
          working-directory: ./tf-config
          run: |
            terraform init --var-file stg.tfvars -input=false
            terraform apply -var-file stg.tfvars -auto-approve -input=false
  
        - name: Fetch Private-Subnets & Connect Lambda-Existing VPC
          run: |
            vpc_id=$(aws eks describe-cluster --name ${{ secrets.EKS_TEST_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
            subnet_ids=$(aws ec2 describe-subnets --filters Name=vpc-id,Values=$vpc_id --query 'Subnets[?MapPublicIpOnLaunch==`false`].SubnetId' --output text)
            subnet_ids=$(echo $subnet_ids | tr -d '\n' | tr ' ' ',')
            security_group_id=$(aws eks describe-cluster --name ${{ secrets.EKS_TEST_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.securityGroupIds[0]' --output text)
            security_group_id=$(echo $security_group_id | tr -d '\n' | tr ' ' ',')
            aws lambda update-function-configuration --function-name eepr${{ github.event.number }}-gl-stg-lambda --vpc-config SubnetIds="$subnet_ids",SecurityGroupIds="$security_group_id"

        - name: Generate kube config
          run: aws eks update-kubeconfig --region us-east-1 --name ${{ secrets.EKS_TEST_CLUSTER_NAME }}
         
        - name: Install VCluster
          run: |
             curl -L -o vcluster "https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64"
             sudo install -c -m 0755 vcluster /usr/local/bin
             sudo rm -f vcluster
    
        - name: Create VCluster
          run: |
              vcluster create eepr${{ github.event.number }} --namespace eepr${{ github.event.number }} --connect=false
  #          vcluster create eepr${{ github.event.number }} --namespace eepr${{ github.event.number }} --connect=false --upgrade -f ./vc-syncer-values/syncer-values.yaml
  #          vcluster create eepr${{ github.event.number }}--namespace eepr${{ github.event.number }} --connect=false --upgrade --extra-values ./vc-syncer-values/syncer-values.yaml
  
        - name: Connect VCluster, Helm (ArgoRollout)
          id: connect-vcluster
          run: |
             vcluster connect eepr${{ github.event.number }} --namespace eepr${{ github.event.number }} -- helm repo add argo https://argoproj.github.io/argo-helm
             vcluster connect eepr${{ github.event.number }} --namespace eepr${{ github.event.number }} -- helm upgrade --install my-release argo/argo-rollouts --set dashboard.enabled=true
             vcluster_endpoint_url=$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}')
             echo "VCLUSTER_ENDPOINT_URL=${vcluster_endpoint_url}"
             echo "##[set-output name=VCLUSTER_ENDPOINT_URL;]${vcluster_endpoint_url}"  
  #          vcluster connect eepr${{ github.event.number }} --namespace eepr${{ github.event.number }} -- kubectl get ns
        
        - name: Connect VCluster, Helm (NGinx)
          run: vcluster connect eepr${{ github.event.number }} --namespace eepr${{ github.event.number }} -- helm upgrade --install nginx-app ./nginx --values ./nginx/values.yaml

        - name: Set up PostgreSQL Client
          run: sudo apt-get install -y postgresql-client

        - name: Set up Bastion-Host SSH
          uses: webfactory/ssh-agent@v0.5.0
          with:
            ssh-private-key: ${{ secrets.BASTION_PRIVATE_KEY }}

        - name: Bastion-Host SSH & Create New RDS-Schema 
          run: |
              ssh -o StrictHostKeyChecking=no -A -L 5432:34.207.98.242:5432 ec2-user@34.207.98.242 'bash -s' << 'EOF'
                export PGPASSFILE=~/gl-notifications-test-rds.pgpass
                sudo chmod 600 gl-notifications-test-rds.pgpass
                sudo chown ec2-user:ec2-user ~/gl-notifications-test-rds.pgpass
                pg_dump -h gl-notifications-test-rds.cluster-c74weq8kegwi.us-east-1.rds.amazonaws.com -p 5432 -U retailtest -d gl-notifications-test-rds -f gl-notifications-test-rds-bkp.dump
                psql -h gl-notifications-test-rds.cluster-c74weq8kegwi.us-east-1.rds.amazonaws.com -p 5432 -U retailtest -d gl-notifications-test-rds -c "CREATE DATABASE eepr${{ github.event.number }}db;"
                psql -h gl-notifications-test-rds.cluster-c74weq8kegwi.us-east-1.rds.amazonaws.com -p 5432 -U retailtest -d eepr${{ github.event.number }}db < gl-notifications-test-rds-bkp.dump
              EOF
#  psql -h gl-notifications-test-rds.cluster-c74weq8kegwi.us-east-1.rds.amazonaws.com -p 5432 -U retailtest -d eepr${{ github.event.number }}db -c "CREATE SCHEMA IF NOT EXISTS eepr${{ github.event.number }}sch;"

    Comment-PR-with-Endpoint:
      needs: Provision-SQS-Lambda-VCluster
      runs-on: ubuntu-latest #                runs-on: self-hosted
      permissions:
        pull-requests: write
      steps:
        - name: Comment PR
          uses: mshick/add-pr-comment@v2
          with:
            message: |
              Your Ephemeral Environment is Created Successfully
              PLEASE, LOGIN FIRST INTO TEST EVN. (EKS TEST CLUSTER) TO ACCESS YOUR PR. ENV.
              So Your PR EndPoint URL is: ${{ needs.Provision-SQS-Lambda-VCluster.outputs.VCLUSTER_ENDPOINT_URL }}